{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recoginition using FaceNet trained on VGGFACE2\n",
    "This code is a trimmed downn version of [facenet-pytroch](https://github.com/timesler/facenet-pytorch). It also uses the pretrained weights from the facenet-pytorch project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "When you start doing this project for the first time, you'll have only an empty folder. Here, we'll\n",
    "- setup repositories\n",
    "    - setup a git repository (You'll have it automatically since you clone an existing repo)\n",
    "    - setup a hangar repository\n",
    "- Add model to stockroom\n",
    "    - Pull our pretrained models\n",
    "    - commit using stockroom\n",
    "- Add data to hangar\n",
    "    - commit using stockroom\n",
    "\n",
    "### Setup repositories\n",
    "Stockroom needs the hangar repository already setup and stockroom relies on git for comprehending the current version. So let's start by setting up a hangar repository in the current folder which is a git repository already. We can use cli of stockroom, `stock` to setup the repository. An example setup is here. You could use `hangar` cli to initialize the hangar repository but `stock` will make sure the hangar repository and git is properly connected\n",
    "\n",
    "```bash\n",
    "stock init --name sherin --email a@b.c\n",
    "```\n",
    "\n",
    "Once the hangar repository is initialized, run the below cell to verify the existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the repository existence\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if not cwd.joinpath('.git').exists():\n",
    "    warnings.warn(\"Git repository does not exist\")\n",
    "if not cwd.joinpath('.hangar').exists():\n",
    "    warnigns.warn(\"hangar repository does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding model to stockroom\n",
    "Using pretrained weights is very common in the deep learning community to avoid training of the huge network from the scratch. For our face recognition model, we'll download the pretrained weights downlaoded from [facenet-pytroch](https://github.com/timesler/facenet-pytorch). Once we have the pretrained weights, we need to add it to hangar using stockroom.\n",
    "The network we use has different components and weights of each of them are saved separately. Here we download all of them and then load it into the runtime using `torch.load_state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cache exists, skipping download\n",
      "File cache exists, skipping download\n",
      "File cache exists, skipping download\n",
      "File cache exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "from utils import download\n",
    "\n",
    "onet_url = 'https://drive.google.com/uc?export=download&id=1dcyEOAa2fc4lILKKDbMWFaBpLyA_7GOe'\n",
    "pnet_url = 'https://drive.google.com/uc?export=download&id=1p-aeR9jQ4kQNrPMVMTC5l_aTVrmJZ5c9'\n",
    "rnet_url = 'https://drive.google.com/uc?export=download&id=1olU2yzLX1g2wQ6sTKqzktb2Q1oyfli4t'\n",
    "resnet_url = 'https://drive.google.com/uc?export=download&id=1TES47D1ZP6NGF2GFw8ZUcKL205L9q3_f'\n",
    "\n",
    "download(onet_url, 'onet.pth', cache=True)\n",
    "download(pnet_url, 'pnet.pth', cache=True)\n",
    "download(rnet_url, 'rnet.pth', cache=True)\n",
    "download(resnet_url, 'resnet.pth', cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Committing it to hangar using stockroom\n",
    "Here we use stockroom's python API to add model to hangar and `stock` cli to commit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import mtcnn\n",
    "from models import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "pnet_wt = {f\"pnet.{key}\": val for key, val in torch.load('pnet.pth').items()}\n",
    "state_dict.update(pnet_wt)\n",
    "rnet_wt = {f\"rnet.{key}\": val for key, val in torch.load('rnet.pth').items()}\n",
    "state_dict.update(rnet_wt)\n",
    "onet_wt = {f\"onet.{key}\": val for key, val in torch.load('onet.pth').items()}\n",
    "state_dict.update(onet_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcnn_model = mtcnn.MTCNN()\n",
    "mtcnn_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/hhsecond/mypro/stockroom/stockroom/storages/modelstore.py(26)save_torch()\n",
      "-> for i, (layer, arr) in enumerate(weights.items()):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pnet.conv1.weight'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  aset_name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_STOCK--_mtcnn'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from stockroom import ModelStore\n",
    "ms = ModelStore('torch')\n",
    "ms['mtcnn'] = mtcnn_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have added data to ModelStore. Let's commit. Remember, if we need this commit to be a part of git history (i.e. we might need to come back to this stage of code and data. not just data), we need to stock commit first. This adds the relavent information to the stock file which is then needs to be commited to git. \n",
    "\n",
    "```bash\n",
    "stock commit -m 'adding mtcnn model'\n",
    "git add head.stock\n",
    "git commit -m 'added mtcnn model'\n",
    "```\n",
    "\n",
    "Once the model weights are commited, we can get it back using the dictionary style access\n",
    "#### Fetching the weights back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out COMMIT: 528e1d249ffafc7ccde04ac8fcc97d0ff21f5dba\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f825d32d8a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mtcnn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmtcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mypro/stockroom/stockroom/storages/modelstore.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mypro/stockroom/stockroom/storages/modelstore.py\u001b[0m in \u001b[0;36mload_torch\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0masetn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraysets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0masetn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maset_substr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails_from_asetkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masetn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# ignoring higher numbers that might have come from another commit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mindx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mypro/stockroom/stockroom/parser.py\u001b[0m in \u001b[0;36mdetails_from_asetkey\u001b[0;34m(asetkey)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetails_from_asetkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masetkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masetkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "state_dict = ms['mtcnn']\n",
    "mtcnn_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But ModelStorage comes with an inbuilt API to do the state_dict loading without you doing that explicitly.\n",
    "#### Using `set_weights` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Neither BRANCH or COMMIT specified.\n",
      " * Checking out writing HEAD BRANCH: master\n"
     ]
    }
   ],
   "source": [
    "ms.set_weights(mtcnn_model, 'mtcnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the resnet model also to stockroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockroom import ModelStore\n",
    "import torch\n",
    "\n",
    "ms = ModelStore('torch')\n",
    "num_classes = 5  # let's assume we knew it before hand\n",
    "resnet_model = resnet.InceptionResnetV1(num_classes=num_classes)\n",
    "state_dict = torch.load('resnet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model `state_dict` does not have the weights for the final layer. The final layer needed to initialize and fine tune based `num_classes`. What is `num_classes`? So the pretrained facenet is good at figuring out key features it needs to recognize a face. But now we need to make this network recognize few of our friends. For that we'll the teach facenet how our friends looks like, by giving it few pictures of our friends. Well, facenet needs to know how many friends does it need to meet before we start the training. The `num_classes` represents this number. Here we are creating a dummy pytorch layer get the weight of the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_linear_layer = torch.nn.Linear(512, num_classes)  # we knew 512 before hand\n",
    "state_dict.update({'logits.weight': dummy_linear_layer.state_dict()['weight']})\n",
    "state_dict.update({'logits.bias': dummy_linear_layer.state_dict()['bias']})\n",
    "\n",
    "resnet_model.load_state_dict(state_dict)\n",
    "state_dict = resnet_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arrayset name provided: `__STOCKROOM--_resnet--_36--_repeat_1.0.branch0.conv.weight--_32_256_1_1` is invalid. Can only contain alpha-numeric or \".\" \"_\" \"-\" ascii characters (no whitespace). Must be <= 64 characters long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e3ef41fd5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resnet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mypro/stockroom/stockroom/storages/modelstore.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, model_name, weights)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mypro/stockroom/stockroom/storages/modelstore.py\u001b[0m in \u001b[0;36msave_torch\u001b[0;34m(self, model_name, weights)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maset_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraysets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 aset = co.arraysets.init_arrayset(\n\u001b[0;32m---> 33\u001b[0;31m                     aset_name, shape=np_arr.shape, dtype=np_arr.dtype)\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0maset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mypro/hangar-py/src/hangar/arrayset.py\u001b[0m in \u001b[0;36minit_arrayset\u001b[0;34m(self, name, shape, dtype, prototype, named_samples, variable_shape, backend_opts)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;31m# ----------- Determine schema format details -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mypro/hangar-py/src/hangar/arrayset.py\u001b[0m in \u001b[0;36minit_arrayset\u001b[0;34m(self, name, shape, dtype, prototype, named_samples, variable_shape, backend_opts)\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mis_suitable_user_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mis_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1313\u001b[0;31m                     \u001b[0;34mf'Arrayset name provided: `{name}` is invalid. Can only contain '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m                     \u001b[0;34mf'alpha-numeric or \".\" \"_\" \"-\" ascii characters (no whitespace). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                     f'Must be <= 64 characters long')\n",
      "\u001b[0;31mValueError\u001b[0m: Arrayset name provided: `__STOCKROOM--_resnet--_36--_repeat_1.0.branch0.conv.weight--_32_256_1_1` is invalid. Can only contain alpha-numeric or \".\" \"_\" \"-\" ascii characters (no whitespace). Must be <= 64 characters long"
     ]
    }
   ],
   "source": [
    "ms['resnet'] = state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data to stockroom\n",
    "Data addition can be done in different ways, we are taking the most obvious here - to read the image from the disk using a package like PIL and add it to stockroom's `DataStore` like we added the model to `ModelStore`.  We will need to make arraysets (arraysets is the fundamental data structure of hangar that stores tensors/arrays. Read more about arraysets [here](https://hangar-py.readthedocs.io/en/stable/concepts.html#abstraction-2-what-makes-up-a-arrayset)). Hangar has CLI defined for making arraysets. Here we make a veriably shaped image arrayset that can take images of different size and a fixed size label arrayset that stores classes/labels\n",
    "\n",
    "```bash\n",
    "hangar arrayset create images UINT8 1000 1000 3 --variable-shape\n",
    "hangar arrayset create label INT64 1\n",
    "hangar commit -m 'arrayset init'\n",
    "```\n",
    "\n",
    "Note: hangar already has a plugin system built in which could make data addition easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "raw_data = Path('raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockroom import DataStore\n",
    "\n",
    "ds = DataStore()\n",
    "i = 0\n",
    "label_dict = {}\n",
    "for label_folder in raw_data.iterdir():\n",
    "    if label_folder.name not in label_dict:\n",
    "        label_dict[label_folder.name] = len(label_dict)\n",
    "    for item in label_folder.iterdir():\n",
    "        arr = np.array(Image.open(item).convert('RGB'))\n",
    "        ds['images', i] = arr\n",
    "        ds['label', i] = np.array([label_dict[label_folder.name]])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now commit the data addition to hangar using stock\n",
    "```bash\n",
    "stock commit -m 'data added'\n",
    "```\n",
    "\n",
    "Ok Great! we have finished the quick \"getting started\" guide. You must have a hangar repository now with the models and data required for training the face recoginition algorithm stored. Let's move on to the next notebook where we actually train a neural network and see how stockroom could run along side your normal work flow while version your data, model, hyperparameters and even metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
